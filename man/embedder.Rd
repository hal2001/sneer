% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sneer.R
\name{embedder}
\alias{embedder}
\title{Create an Embedding Method}
\usage{
embedder(cost, kernel, kappa = 0.5, lambda = 0.5, beta = 1, alpha = 0,
  dof = 1, norm = "joint", importance_weight = FALSE, verbose = TRUE)
}
\arguments{
\item{cost}{The cost function to optimize. See 'Details'. Can be abbreviated.}

\item{kernel}{The function used to convert squared distances to weights. See
'Details'. Can be abbreviated.}

\item{kappa}{Controls the weighting of the \code{"js"} cost function. Must
take a value between 0 (where it behaves like \code{"KL"}) and 1 (where it
behaves like \code{"reverse-KL"}).}

\item{lambda}{Controls the weighting of the \code{"nerv"} cost function. Must
take a value between 0 (where it behaves like \code{"reverse-KL"}) and 1
(where it behaves like \code{"KL"}).}

\item{beta}{Precision (narrowness) of the \code{"exponential"} and
\code{"heavy-tailed"} kernels.}

\item{alpha}{Heavy tailedness of the \code{"heavy-tailed"} kernel. A value of
0 makes the kernel behave like \code{"exponential"}, and a value of 1
behaves like \code{"heavy-tailed"}.}

\item{dof}{Degrees of freedom of the \code{"inhomogeneous"} kernel. A value
of 1 makes the kernel behave like \code{"t-distributed"}, and a value
approaching approaching infinity behaves like \code{"exponential"}.}

\item{norm}{Weight normalization to carry out. See 'Details'.
Can be abbreviated.}

\item{importance_weight}{If \code{TRUE}, modify the embedder to use the
importance weighting method (Yang et al. 2014).}

\item{verbose}{If \code{TRUE}, log information about the embedding method to
the console.}
}
\value{
An embedding method, to be passed as an argment to the \code{method}
  parameter of \code{\link{sneer}}.
}
\description{
Creates an embedding method to be used in the \code{\link{sneer}} function,
allowing arbitrary combinations of cost function, kernel and normalization
schemes. Several embedding methods from the literature (e.g. SNE, t-SNE, JSE,
NeRV) can be created.
}
\details{
The \code{cost} parameter is the cost function to minimize, one of:

\itemize{
\item \code{"KL"} Kullback-Leibler divergence, as used in the asymmetric
Stochastic Neighbor Embedding (SNE) method (Hinton and Roweis, 2002) and
Symmetric Stochastic Neighbor Embedding (SSNE) method (Cook et al., 2007),
and t-distributed SNE (van der Maaten and Hinton,, 2008).
\item \code{"reverse-KL"} Kullback-Leibler divergence, with the output
probability as the reference distribution. Part of the cost function used in
the Neighbor Retrieval Visualizer (NeRV) method (Venna et al., 2010).
\item \code{"nerv"} Cost function used in the (NeRV) method (Venna et al.,
2010).
\item \code{"JS"} Jensen-Shannon divergence, as used in the Jensen-Shannon
Embedding (JSE) method (Lee et al., 2013).
}

The \code{kernel} is a function that transform squared output distances
into weights. Must be one of:

\itemize{
\item \code{"exponential"} Exponential function as used in the asymmetric
Stochastic Neighbor Embedding (SNE) method (Hinton and Roweis, 2002) and
Symmetric Stochastic Neighbor Embedding (SSNE) method (Cook et al., 2007).
\item \code{"t-distributed"} The t-distribution with one degree of freedom,
as used in t-distributed SNE (van der Maaten and Hinton,, 2008).
\item \code{"heavy-tailed"}. Heavy-tailedness function used in Heavy-tailed
SSNE (Zhang et al. 2009).
\item \code{"inhomogeneous"}. The function used in inhomogeneous t-SNE
(Kitazono et al. 2016).
}

The \code{normalization} determines how weights are converted to
probabilities. Must be one of:

\itemize{
  \item \code{"point"} Point-wise normalization, as used in asymmetric SNE,
  NeRV and JSE.
  \item \code{"pair"} Pair-wise normalization.
  \item \code{"joint"} Pair-wise normalization, plus enforcing the
  probabilities to be joint by averaging, as used in symmetric SNE and
  t-distributed SNE. Output probabilities will only be averaged if the
  \code{kernel} has non-uniform parameters.
}

You may also specify a vector of size 2, where the first member is the input
normalization, and the second the output normalization. This should only be
used to mix \code{"pair"} and \code{"joint"} normalization schemes.
}
\examples{
# t-SNE
embedder(cost = "kl", kernel = "t-dist", norm = "joint")

# NeRV
embedder(cost = "nerv", kernel = "exp", norm = "point")

# JSE
embedder(cost = "JS", kernel = "exp", norm = "point")

# weighted SSNE
embedder(cost = "kl", kernel = "exp", norm = "joint", importance_weight = TRUE)

# SSNE where the input probabilities are averaged, but output probabilites
# are not. This only has an effect if the kernel parameters are set to be
# non-uniform.
embedder(cost = "kl", kernel = "exp", norm = c("joint", "pair"))
\dontrun{
# Pass result of calling embedder to the sneer function's method parameter
sneer(iris, method = embedder(cost = "kl", kernel = "t-dist", norm = "joint"))
}
}
\references{
Cook, J., Sutskever, I., Mnih, A., & Hinton, G. E. (2007).
Visualizing similarity data with a mixture of maps. In \emph{International
Conference on Artificial Intelligence and Statistics} (pp. 67-74).

Hinton, G. E., & Roweis, S. T. (2002). Stochastic neighbor embedding. In
\emph{Advances in neural information processing systems} (pp. 833-840).

Kitazono, J., Grozavu, N., Rogovschi, N., Omori, T., & Ozawa, S. (2016,
October). t-Distributed Stochastic Neighbor Embedding with Inhomogeneous
Degrees of Freedom. In \emph{International Conference on Neural Information
Processing (ICONIP 2016)} (pp. 119-128). Springer International Publishing.

Lee, J. A., Renard, E., Bernard, G., Dupont, P., & Verleysen, M. (2013). Type
1 and 2 mixtures of Kullback-Leibler divergences as cost functions in
dimensionality reduction based on similarity preservation.
\emph{Neurocomputing}, \emph{112}, 92-108.

Van der Maaten, L., & Hinton, G. (2008). Visualizing data using t-SNE.
\emph{Journal of Machine Learning Research}, \emph{9}(2579-2605).

Venna, J., Peltonen, J., Nybo, K., Aidos, H., & Kaski, S. (2010). Information
retrieval perspective to nonlinear dimensionality reduction for data
visualization. \emph{Journal of Machine Learning Research}, \emph{11},
451-490.

Yang, Z., King, I., Xu, Z., & Oja, E. (2009). Heavy-tailed symmetric
stochastic neighbor embedding. In \emph{Advances in neural information
processing systems} (pp. 2169-2177).

Yang, Z., Peltonen, J., & Kaski, S. (2014). Optimization equivalence of
divergences improves neighbor embedding. In \emph{Proceedings of the 31st
International Conference on Machine Learning (ICML-14)} (pp. 460-468).
}
\seealso{
For literature embedding methods, \code{\link{sneer}} will generate
  the method for you, by passing its name (e.g. \code{method = "tsne"}). This
  function is only strictly necessary for experimentation purposes.
}
