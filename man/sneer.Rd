% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/embed.R, R/sneer.R
\docType{package}
\name{sneer}
\alias{sneer}
\alias{sneer-package}
\title{Probability-based Embedding}
\usage{
sneer(df, indexes = NULL, method = "tsne", alpha = 0.5, lambda = 0.5,
  kappa = 0.5, scale_type = "", perplexity = 32, perp_scale = "single",
  perp_scale_iter = NULL, perp_kernel_fun = "exp", prec_scale = "",
  init = "p", init_config = NULL, opt = "NAG-BOLD", epsilon = 1,
  max_iter = 1000, report_every = 50, tol = 1e-04, exaggerate = NULL,
  exaggerate_off_iter = 50, plot_type = "p", colors = NULL,
  color_name = NULL, labels = NULL, label_name = NULL,
  label_chars = NULL, point_size = 1, plot_labels = FALSE,
  color_scheme = grDevices::rainbow, equal_axes = FALSE, legend = TRUE,
  legend_rows = NULL, quality_measures = NULL, ret = c())
}
\arguments{
\item{df}{Data frame or distance matrix (as dist object) to embed.}

\item{indexes}{Indexes of the columns of the numerical variables to use in
the embedding. The default of \code{NULL} will use all the numeric
variables.}

\item{method}{Embedding method. See 'Details'.}

\item{alpha}{Heavy tailedness parameter. Used only if the method is
\code{"hssne"}.}

\item{lambda}{NeRV parameter. Used only if the method is \code{"nerv"}.}

\item{kappa}{JSE parameter. Used only if the method is \code{"jse"}.}

\item{scale_type}{Type of scaling to carry out on the input data. See
'Details'.}

\item{perplexity}{Target perplexity or vector of trial perplexities (if
\code{perp_scale} is set). Applies to probability-based embedding methods
only (i.e. anything that isn't PCA, MDS or Sammon mapping).}

\item{perp_scale}{Type of perplexity scaling to apply. See 'Details'. Ignored
by non-probability based methods.}

\item{perp_scale_iter}{Number of iterations to scale perplexity values over.
Must be smaller than the \code{max_iter} parameter. Default is to use
\code{max_iter / 5}. Ignored by non-probability based methods or if
\code{perp_scale} is not set.}

\item{perp_kernel_fun}{The input data weight function. Either \code{"exp"}
to use exponential function (the default) or \code{"step"} to use a step
function. The latter emulates a k-nearest neighbor graph, but does not
provide any of the efficiency advantages of a sparse matrix.}

\item{prec_scale}{Whether to scale the output kernel precision based on
perplexity results. See 'Details'. Ignored by non-probability based methods.
Can't be used if \code{perp_kernel_fun} is set to \code{"step"}.}

\item{init}{Type of initialization of the output coordinates. See 'Details'.}

\item{init_config}{Coordinates to use for initial configuration. Used only
if \code{init} is \code{"m"}.}

\item{opt}{Type of optimizer. See 'Details'.}

\item{epsilon}{Learning rate. Used only when \code{opt} is set to
\code{"TSNE"}.}

\item{max_iter}{Maximum number of iterations to carry out optimization of
the embedding. Ignored if the \code{method} is \code{"pca"}.}

\item{report_every}{Frequency (in terms of iteration number) with which to
update plot and report the cost function.}

\item{tol}{Tolerance for comparing cost change (calculated according to the
interval determined by \code{report_every}). If the change falls below this
value, the optimization stops early.}

\item{exaggerate}{If non-\code{NULL}, scales input probabilities by this
value from iteration 0 until \code{exaggerate_off_iter}. Normally a value
of \code{4} is used. Has no effect with PCA, Sammon mapping or metric MDS.
Works best when using random initialization with (\code{init = "r"} or
\code{"u"}. You probably don't want to use it if you are providing your own
initial configuration (\code{init = "m"}).}

\item{exaggerate_off_iter}{Iteration number to stop the "early exaggeration"
scaling specified \code{exaggerate}. Has no effect if \code{exaggerate} is
\code{NULL}.}

\item{plot_type}{String code indicating the type of plot of the embedding
to display: \code{"p"} to use the usual \code{\link[graphics]{plot}}
function; \code{"g"} to use the \code{ggplot2} package. You are responsible
for installing and loading the ggplot2 package yourself.}

\item{colors}{Vector of colors to use to color each point in the embedding
plot.}

\item{color_name}{Name of column of colors in \code{df} to be used to color
the points directly. Ignored if \code{colors} is provided.}

\item{labels}{Factor vector associated with (but not necessarily in)
\code{df}. Used to map from factor levels to colors in the embedding plot
(if no \code{color} or \code{color_name} is provided), and as text labels
in the plot if \code{plot_labels} is \code{TRUE}. Ignored if \code{colors}
or \code{color_name} is provided.}

\item{label_name}{Name of a factor column in \code{df}, to be used like
\code{labels}. Ignored if \code{labels} is provided.}

\item{label_chars}{Number of characters to use for the labels in the
embedding plot. Applies only when \code{plot_type} is set to \code{"p"}.}

\item{point_size}{Size of the points (or text) in the embedding plot.}

\item{plot_labels}{If \code{TRUE} and either \code{labels} or
\code{label_name} is provided, then the specified factor column will be used
to provide a text label associated with each point in the plot. Only useful
for small dataset with short labels. Ignored if \code{plot_type} is not
set to \code{"p"}.}

\item{color_scheme}{Either a color ramp function, or the name of a Color
Brewer palette name to use for mapping the factor specified by
\code{labels} or \code{label_name}. Ignored if not using \code{labels}
or \code{label_name}.}

\item{equal_axes}{If \code{TRUE}, the embedding plot will have the axes
scaled so that both the X and Y axes have the same extents. Only applies if
\code{plot_type} is set to \code{"p"}.}

\item{legend}{if \code{TRUE}, display the legend in the embedding plot.
Applies when \code{plot_type} is \code{"g"} only.}

\item{legend_rows}{Number of rows to use for displaying the legend in
an embedding plot.}

\item{quality_measures}{Vector of names of quality measures to apply to the
finished embedding. See 'Details'. Values of the quality measures will
be printed to screen after embedding and retained in the list that is
returned from this function.}

\item{ret}{Vector of names of extra data to return from the embedding. See
'Details',}
}
\value{
List with the following elements:
\itemize{
\item \code{coords} Embedded coordinates.
\item \code{cost} Cost function value for the embedded coordinates. The
 type of the cost depends on the method, but the lower the better.
\item \code{norm_cost} \code{cost}, normalized so that a perfect embedding
 gives a value of 0 and one where all the distances were equal would have
 a value of 1.
\item \code{method} String giving the method used for the embedding.
}
Additional elements will be in the list if \code{ret} or
\code{quality_measures} are non-empty.
}
\description{
Carries out an embedding of a high-dimensional dataset into a two dimensional
scatter plot, based on distance-based methods (e.g. Sammon maps) and
probability-based methods (e.g. t-distributed Stochastic Neighbor Embedding).

A package for exploring probability-based embedding and related forms
of dimensionality reduction. Its main goal is to implement multiple
embedding methods within a single framework so comparison between them is
easier, without worrying about the effect of differences in preprocessing,
optimization and heuristics.
}
\details{
The embedding methods available are:
\itemize{
 \item \code{"pca"} The first two principal components.
 \item \code{"mmds"} Metric multidimensional scaling.
 \item \code{"sammon"} Sammon map.
 \item \code{"tsne"} t-Distributed Stochastic Neighbor Embedding of van der
  Maaten and Hinton (2008).
 \item \code{"asne"} Asymmetric Stochastic Neighbor Embedding of Hinton and
  Roweis (2002).
 \item \code{"ssne"} Symmetric Stochastic Neighbor Embedding of Cook et al
  (2007).
 \item \code{"wssne"} Weighted Symmetric Stochastic Neighbor Embedding of
  Yang et al (2014). Note that despite its name this version is a
  modification of t-SNE, not SSNE.
 \item \code{"hssne"} Heavy-tailed Symmetric Stochastic Neighbor Embedding of
  Yang et al (2009).
 \item \code{"nerv"} Neighbor Retrieval Visualizer of Venna et al (2010).
 \item \code{"jse"} Jensen-Shannon Embedding of Lee at al (2013).
}

The following scaling options can be applied via the \code{scale_type}
parameter:
\itemize{
 \item \code{"m"} Range scale the entire data so that the maximum value is
  1 and the minimum 0.
 \item \code{"r"} Range scale each column that the maximum value in each
  column is 1 and the minimum 0.
 \item \code{"a"} Scale each column so that its mean is 0 and variance is
  1.
}
Default is to do no scaling. Zero variance columns will be removed even if no
preprocessing is carried out.

The \code{perplexity} parameter is used in combination with the
\code{perp_scale} parameter, which can take the following values:
\itemize{
 \item \code{"single"} \code{perplexity} should be a single value, which
   will be used over the entire course of the embedding.
 \item \code{"step"} \code{perplexity} should be a vector of
    perplexity values. Each perplexity will be used in turn over the course
    of the embedding, in sequential order. By starting with a large
    perplexity, and ending with the desired perplexity, it has been
    suggested by some researchers that local minima can be avoided.
\item \code{"multi"} The multiscaling method of Lee et al (2015).
    \code{perplexity} should be a vector of perplexity values. Each
    perplexity will be used in turn over the course of the embedding, in
    sequential order. Unlike with the \code{"step"} method, probability
    matrices from earlier perplexities are retained and combined by
    averaging.
}

For \code{perp_scale} values that aren't \code{"single"}, if a non-vector
argument is suppied to the \code{perplexity} argument, it will be ignored,
and a suitable vector of perplexity values will be used instead. For
\code{"multi"} these will range from the the number of observations in the
dataset divided by four down to 2, in descending powers of 2. For
\code{"step"}, 5 equally spaced values ranging from the number of
observations divided by 2 down to 32 (or the number of observations divided
by 4, if the dataset is smaller than 65 observations.)

The \code{prec_scale} parameter determines if the input weighting kernel
parameters should be used to modify the output kernel parameter after the
input probability calculation for a given perplexity value completes.
values are:
\itemize{
 \item \code{"n"} Do nothing. Most embedding methods follow this strategy,
   leaving the output similarity kernels to all have unit precision
   parameters.
 \item \code{"t"} Transfer the input similarity kernel parameters to the
   output similarity kernel. This method was suggesed by Venna et al (2010).
 \item \code{"s"} Scale the output kernel precisions based on the target
   \code{perplexity} and the intrinsic dimensionality of the input data. This
   method is part of the multiscaling technique proposed by Lee et al (2015).
}

The \code{prec_scale} parameter will be ignored if the \code{method} used
does not use an output similarity kernel with a free parameter, e.g.
\code{tsne} or \code{wtsne}. Also, because the input and output similarity
kernels must be of the same type, \code{prec_scale} is incompatible with
setting \code{perp_kernel_fun} to "step".

For initializing the output coordinates, the options for the
\code{init} parameter are:
\itemize{
 \item \code{"p"} Initialize using the first two scores of the PCA (using
 classical MDS if \code{df} is a distance matrix). Data will be centered,
 but not scaled unless the \code{scale_type} parameter is used.
 \item \code{"r"} Initialize each coordinate value from a normal random
 distribution with a standard deviation of 1e-4, as suggested by van der
 Maaten and Hinton (2008).
 \item \code{"u"} Initialize each coordinate value from a uniform random
 distribution between 0 and 1 as suggested by Venna et al (2010).
 \item \code{"m"} Initialize the coordinates from a user-supplied matrix.
  Supply the coordinates as the \code{init_config} parameter.
}

For configuring the optimization method, the options for the \code{opt}
parameter are:
\itemize{
 \item \code{"TSNE"} The optimization method used in the original t-SNE
   paper: the Jacobs method for step size selection and a step function
   for the momentum: switching from 0.4 to 0.8 after 250 steps. You may need
   to modify the \code{"epsilon"} parameter to get good results, depending
   on how you have scaled and preprocessed your data, and the embedding
   method used.
 \item \code{"L-BFGS"} The low-memory BFGS method.
 \item \code{"NAG-BOLD"} Nesterov Accelerated Gradient with bold driver step
   size selection.
 \item \code{"NAG-MT"} Nesterov Accelerated Gradient with More-Thuente step
   size selection.
 \item \code{"NAG-R"} Nesterov Accelerated Gradient with Rasmussen step size
  selection.
 \item \code{"CG-MT"} Conjugate Gradient with More-Thuente step size
 selection.
 \item \code{"CG-R"} Conjugate Gradient with Rasmussen step size selection.
 \item \code{"SPEC-BOLD"} Spectral Direction method of Vladymyrov and
 Carreira-Perpinan (2012) with bold driver step size selection.
 \item \code{"SPEC-MT"} Spectral Direction with More-Thuente step size
 selection.
 \item \code{"SPEC-R"} Spectral Direction with Rasmussen step size selection.
}

There are some caveats to using these optimization routines:

\itemize{
 \item To use the conjugate gradient method or the
  Rasmussen or More-Thuente step size methods, you must install and load the
  \code{rconjgrad} package from \url{https://github.com/jlmelville/rconjgrad}.
 \item The external optimization routines (\code{L-BFGS} and \code{CG-}
  methods) run in batches of \code{report_every}. For example, if you want to
  report every 50 iterations, the optimization routine will be run for 50
  iterations, the cost is logged to screen, and then a new batch of 50
  iterations are run, losing any memory of the previous direction or other
  state, effectively "resetting" the search. Therefore, do not set
  \code{report_every} too low in this case, or the optimization will
  approach the behavior of steepest descent.
 \item Use of the external optimization routines is incompatible with
  \code{perp_scale} settings that need to update the input probabilities at
  certain iterations (e.g. multiscaling), because that iteration number might
  have been "lost" inside the optimization routine.
 \item The spectral direction method requires a probability-based embedding
  method and that the input probability matrix be symmetric. Some
  probability-based methods are not compatible (e.g. NeRV and JSE; t-SNE
  works with it, however). Also, while it works with the dense matrices used
  by sneer, because this method uses a Cholesky decomposition of the input
  probability matrix which has a complexity of O(N^3), it is intended
  to be used with sparse matrices. Its inclusion here is suitable for use
  with smaller datasets.
}

The default is to use NAG with the bold driver step size, and adaptive
restarting. This is not quite as fast as using the Jacobs method for some
datasets, but is more robust across different embedding methods and scaling,
and doesn't require fiddling with the learning rate.

For the \code{quality_measures} argument, a vector with one or more of the
following options can be supplied:
\itemize{
 \item \code{"r"} Calculate the area under the ROC curve, averaged over
  each observation, using the output distance matrix to rank each
  observation. Observations are partitioned into the positive and negative
  class depending upon the value of the label determined by the
  \code{label_name} argument. Only calculated if the \code{label_name}
  parameter is supplied.
 \item \code{"p"} Calculate the area under the Precision-Recall curve.
  Only calculated if the \code{label_name} parameter is supplied.
 \item \code{"n"} Calculate the area under the RNX curve, using the
  method of Lee et al (2015).
}

Progress of the embedding is logged to the standard output every 50
iterations. The raw cost of the embedding will be provided along with some
tolerances of either how the embedding or the cost has changed.

Because the different costs are not always scaled in a way that makes it
obvious how well the embedding has performed, a normalized cost is also
shown, where 0 is the minimum possible cost (coinciding with the
probabilities or distances in the input and output space being matched), and
a normalized cost of 1 is what you would get if you just set all the
distances and probabilities to be equal to each other (i.e. ignoring any
information from the input space).

Also, the embedding will be plotted. Plotting can be done
with either the standard \code{\link[graphics]{plot}} function (the default
or by explicitly providing \code{plot_type = "p"}) or with the \code{ggplot2}
library (which you need to install and load yourself), by using
\code{plot_type = "g"}. The goal has been to provide enough customization to
give intelligible results for most datasets. The following are things to
consider:

\itemize{
 \item The plot symbols are normally filled circles. However, if you
 set the \code{plot_text} argument to \code{TRUE}, the \code{labels}
 argument can be used to provide a factor vector that provides a meaningful
 label for each data point. In this case, the text of each factor level will
 be used as a level. This creates a mess with all but the shortest labels
 and smallest datasets. There's also a \code{label_fn} parameter that lets
 you provide a function to convert the vector of labels to a different
 (preferably shorter) form, but you may want to just do it yourself ahead
 of time and add it to the data frame.
 \item Points are colored using two strategies. The most straightforward way
 is to provide a vector of rgb color strings as an argument to \code{colors}.
 Each element of \code{colors} will be used to color the equivalent point
 in the data frame. Note, however, this is currently ignored when plotting
 with ggplot2.
 \item The second way to color the embedding plot uses the \code{labels}
 parameter mentioned above. Each level of the factor used for \code{labels}
 will be mapped to a color and that used to color each point. The mapping
 is handled by the \code{color_scheme} parameter. It can be either a color
 ramp function like \code{\link[grDevices]{rainbow}} or the name of a color
 scheme in the \code{RColorBrewer} package (e.g. \code{"Set3"}). The latter
 requires the \code{RColorBrewer} package to have been installed and loaded.
 Unlike with using \code{colors}, providing a \code{labels} argument works
 with ggplot2 plots. In fact, you may find it preferable to use ggplot2,
 because if the \code{legend} argument is \code{TRUE} (the default), you
 will get a legend with the plot. Unfortunately, getting a legend with an
 arbitary number of elements to fit on an image created with the
 \code{graphics::plot} function and for it not to obscure the points proved
 beyond my capabilities. Even with ggplot2, a dataset with a large number
 of categories can generate a large and unwieldy legend.
}

Additionally, instead of providing the vectors directly, there are
\code{color_name} and \code{label_name} arguments that take a string
containing the name of a column in the data frame, e.g. you can use
\code{labels = iris$Species} or \code{label_name = "Species"} and get the
same result.

If you don't care that much about the colors, provide none of these options
and sneer will try and work out a suitable column to use. If it finds at
least one color column in the data frame (i.e. a string column where every
element can be parsed as a color), it will use the last column found as
if you had provided it as the \code{colors} argument.

Otherwise, it will repeat the process but looking for a vector of factors.
If it finds one, it will map it to colors via the \code{color_scheme}, just
as if you had provided the \code{labels} argument. The default color scheme
is to use the \code{rainbow} function so you should normally get a colorful,
albeit potentially garish, result.

For the \code{ret} argument, a vector with one or more of the
following options can be supplied:
\itemize{
 \item \code{"x"} The input coordinates after scaling and column filtering.
 \item \code{"dx"} The input distance matrix. Calculated if not present.
 \item \code{"dy"} The output distance matrix. Calculated if not present.
 \item \code{"p"} The input probability matrix.
 \item \code{"q"} The output probability matrix.
 \item \code{"prec"} The input similarity kernel precisions.
 \item \code{"dim"} The intrinsic dimensionality for each observation,
 calculated according to the method of Lee et al (2015). These are
 meaningless if not using the default exponential \code{perp_kernel_fun}.
 \item \code{"deg"} Degree centrality of the input probability. Calculated
 if not present.
}

The \code{color_scheme} parameter is used to set the color scheme for the
embedding plot that is displayed during the optimization. It can be one of
either a color ramp function (e.g. \code{grDevices::rainbow}), accepting an
integer n as an argument and returning n colors, or the name of a ColorBrewer
color scheme (e.g. "Spectral"). Using a ColorBrewer scheme requires the
\code{RColorBrewer} package be installed.

For some applicable color ramp functions, see the \code{Palettes} help page
in the \code{grDevices} package (e.g. by running the \code{?rainbow} command).
}
\section{Embedding}{


The \code{\link{sneer}} function provides a variety of methods for embedding,
including:

\itemize{
 \item{Stochastic Neighbor Embedding and variants (ASNE, SSNE and TSNE)}
 \item{Metric MDS using the STRESS and SSTRESS functions}
 \item{Sammon Mapping}
 \item{Heavy-tailed Symmetric Stochastic Neighbour Embedding (HSSNE)}
 \item{Neigbor Retrieval Visualizer (NeRV)}
 \item{Jensen-Shannon Embedding (JSE)}
}

See the documentation for the function for the exact list of methods
and variations.

Optimization is carried out using the momentum interpretation
of the Nesterov Accelerated Gradient method (Sutskever et al 2013.) with
an adaptive restart (O'Donoghue and Candes 2013). This seems a bit more
robust compared to the usual t-SNE optimization method across the different
methods exposed by sneer.

Some other optimization methods are also available:
\itemize{
 \item{The low-memory BFGS method from the \code{optim} function.}
 \item{A Polak-Ribiere Conjugate Gradient optimizer, if you install and load
 the rconjgrad package from https://github.com/jlmelville/rconjgrad}
 \item{If you install rconjgrad, you can also use the two line search algorithms
 it implements with the NAG optimizer, instead of the bold driver approach.}
 \item{The Spectral Direction method of Vladymyrov and Carreira-Perpinan.
 This can only be applied to probability-based embedding methods that use a
 symmetric input probability matrix (not NeRV or JSE, but SSNE and t-SNE
 are fine), but works very well. However, it is intended to be used with
 sparse matrices, because it internally uses a Cholesky decomposition of the
 probability matrix, which has a complexity of O(N^3). Currently, lack of
 support for sparse matrices in sneer restricts its applicability to smaller
 datasets. This can be used with the bold driver line search method, but
 to be on the safe side, you might want to also consider the line search
 methods provided by the rconjgrad package.}
}
}

\section{Visualization}{


The \code{\link{embed_plot}} function will take the output of the
\code{\link{sneer}} function and provide a visualization of the embedding.
If you install the \code{RColorBrewer} package installed, you can use the
ColorBrewer palettes by name.
}

\section{Quantifying embedding quality}{


Some functions are available for attempting to quantify embedding quality,
independent of the particular loss function used for an embedding method.
The \code{\link{nbr_pres}} function will measure how well the embedding
preserves a neighborhood of a given size around each observation. The
\code{\link{rnx_auc_embed}} function implements the Area Under the Curve
of the RNX curve (Lee et al. 2015), which generalizes the neighborhood
preservation to account for all neighborhood sizes, with a bias towards
smaller neighborhoods.

If your observations have labels which could be used for a classification
task, then there are also functions which will use these labels to calculate
the Area Under the ROC or PR (Precision/Recall) Curve, using the embedded
distances to rank each observation: these are \code{\link{roc_auc_embed}}
and \code{\link{pr_auc_embed}} functions, respectively. Note that to use
these two functions, you must have the \code{PRROC} package installed.
}

\section{Synthetic Dataset}{

There's a synthetic dataset in this package, called \code{s1k}. It consists
of a 1000 points representing a fuzzy 9D simplex. It's intended to
demonstrate the "crowding effect" and require the sort of
probability-based embedding methods provided in this package (PCA does a
horrible job of separated the 10 clusters in the data). See \code{s1k}
for more details.
}
\examples{
\dontrun{
  # PCA on iris dataset and plot result using Species label name
  res <- sneer(iris, indexes = 1:4, label_name = "Species", method = "pca")
  # Same as above, but with sensible defaults (use all numeric columns, plot
  # with first factor column found)
  res <- sneer(iris, method = "pca")

  # Can use a distance matrix as input with external vector of labels
  res <- sneer(dist(iris[1:4]), method = "pca", labels = iris$Species)

  # scale columns so each one has mean 0 and variance 1
  res <- sneer(iris, method = "pca", scale_type = "a")
  # full species name on plot is cluttered, so just use the first two
  # letters and half size
  res <- sneer(iris, method = "pca", scale_type = "a", label_chars = 2,
               point_size = 0.5)

  library(ggplot2)
  library(RColorBrewer)
  # Use ggplot2 and RColorBrewer palettes for the plot
  res <- sneer(iris, method = "pca", scale_type = "a", plot_type = "g")
  # Use a different ColorBrewer palette, bigger points, and range scale each
  # column
  res <- sneer(iris, method = "pca", scale_type = "r", plot_type = "g",
               color_scheme = "Dark2", point_size = 2)

  # metric MDS starting from the PCA
  res <- sneer(iris, method = "mmds", scale_type = "a", init = "p")
  # Sammon map starting from random distribution
  res <- sneer(iris, method = "sammon", scale_type = "a", init = "r")

  # TSNE with a perplexity of 32, initialize from PCA
  res <- sneer(iris, method = "tsne", scale_type = "a", init = "p",
               perplexity = 32)
  # default settings are to use TSNE with perplexity 32 and initialization
  # from PCA so the following is the equivalent of the above
  res <- sneer(iris, scale_type = "a")

  # Use the standard tSNE optimization method (Jacobs step size method) with
  # step momentum. Range scale the matrix and use an aggressive learning
  # rate (epsilon).
  res <- sneer(iris, scale_type = "m", perplexity = 25, opt = "tsne",
               epsilon = 500)

  # Use the L-BFGS optimization method
  res <- sneer(iris, scale_type = "a", opt = "L-BFGS")
  # Use the Spectral Directions method with bold driver
  res <- sneer(iris, scale_type = "a", opt = "SPEC-BOLD")

  # Load the rconjgrad library: make use of other line search algorithms and
  # conjugate gradient optimizer
  install.packages("devtools")
  devtools::install_github("jlmelville/rconjgrad")
  library("rconjgrad")
  # Use More-Thuente line search with NAG optimizer instead of bold driver
  res <- sneer(iris, scale_type = "a", opt = "NAG-MT")
  # Use Rasmussen line search
  res <- sneer(iris, scale_type = "a", opt = "NAG-R")
  # Use Conjugate Gradient with More-Thuente line search
  res <- sneer(iris, scale_type = "a", opt = "CG-MT")

  # Use the Spectral Direction method with More-Thuente line search
  res <- sneer(iris, scale_type = "a", opt = "SPEC-MT")

  # NeRV method, starting at a more global perplexity and slowly stepping
  # towards a value of 32 (might help avoid local optima)
  res <- sneer(iris, scale_type = "a", method = "nerv", perp_scale = "step")

  # NeRV method has a lambda parameter - closer to 1 it gets, the more it
  # tries to avoid false positives (close points in the map that aren't close
  # in the input space):
  res <- sneer(iris, scale_type = "a", method = "nerv", perp_scale = "step",
               lambda = 1)

  # Original NeRV paper transferred input exponential similarity kernel
  # precisions to the output kernel, and initialized from a uniform random
  # distribution
  res <- sneer(iris, scale_type = "a", method = "nerv", perp_scale = "step",
               lambda = 1, prec_scale = "t", init = "u")

  # Like NeRV, the JSE method also has a controllable parameter that goes
  # between 0 and 1, called kappa. It gives similar results to NeRV at 0 and
  # 1 but unfortunately the opposite way round! The following gives similar
  # results to the NeRV embedding above:
  res <- sneer(iris, scale_type = "a", method = "jse", perp_scale = "step",
               kappa = 0)

  # Rather than step perplexities, use multiscaling to combine and average
  # probabilities across multiple perplexities. Output kernel precisions
  # can be scaled based on the perplexity value (compare to NeRV example
  # which transferred the precision directly from the input kernel)
  res <- sneer(iris, scale_type = "a", method = "jse", perp_scale = "multi",
               prec_scale = "s")

  # HSSNE has a controllable parameter, alpha, that lets you control how
  # much extra space to give points compared to the input distances.
  # Setting it to 1 is equivalent to TSNE, so 1.1 is a bit of an extra push:
  res <- sneer(iris, scale_type = "a", method = "hssne", alpha = 1.1)

  # wTSNE treats the input probability like a graph where the probabilities
  # are weighted edges and adds extra repulsion to nodes with higher degrees
  res <- sneer(iris, scale_type = "a", method = "wtsne")

  # can use a step-function input kernel to make input probability more like
  # a k-nearest neighbor graph (but note that we don't take advantage of the
  # sparsity for performance purposes, sadly)
  res <- sneer(iris, scale_type = "a", method = "wtsne",
               perp_kernel_fun = "step")

  # Some quality measures are available to quantify embeddings
  # The area under the RNX curve measures whether neighbors in the input
  # are still neighors in the output space
  res <- sneer(iris, scale_type = "a", method = "wtsne",
               quality_measures =  c("n"))

  # Create a 5D gaussian with its own column specifying colors to use
  # for each point (in this case, random)
  g5d <- data.frame(matrix(rnorm(100 * 5), ncol = 5),
                    color = rgb(runif(100), runif(100), runif(100)),
                    stringsAsFactors = FALSE)
  # Specify the name of the color column and the plot will use it rather than
  # trying to map factor levels to colors
  res <- sneer(g5d, method = "pca", color_name = "color")

  # If your dataset labels divide the data into natural classes, can
  # calculate average area under the ROC and/or precision-recall curve too,
  # but you need to have installed the PRROC package.
  # All these techniques can be slow (scale with the square of the number of
  # observations).
  library(PRROC)
  res <- sneer(iris, scale_type = "a", method = "wtsne",
               quality_measures =  c("n", "r", "p"))

  # export the distance matrices and do whatever quality measures we
  # want at our leisure
  res <- sneer(iris, scale_type = "a", method = "wtsne", ret = c("dx", "dy"))

  # Calculate the Area Under the Precision Recall Curve for the embedding
  pr <- pr_auc_embed(res$dy, iris$Species)

  # Similarly, for the ROC curve:
  roc <- roc_auc_embed(res$dy, iris$Species)

  # export degree centrality, input weight function precision parameters,
  # and intrinsic dimensionality
  res <- sneer(iris, scale_type = "a", method = "wtsne",
               ret = c("deg", "prec", "dim"))

  # Plot the embedding as points colored by category, using the rainbow
  # color ramp function:
  embed_plot(res$coords, iris$Species, color_scheme = rainbow)

  # Load the RColorBrewer Library
  library(RColorBrewer)

  # Use a ColorBrewer Qualitative color scheme name (pass a string, not
  # a function!)
  embed_plot(res$coords, iris$Species, color_scheme = "Dark2")

  # Visualize embedding colored by various values:
  # Degree centrality
  embed_plot(res$coords, x = res$deg)
  # Intrinsic Dimensionality using the PRGn palette
  embed_plot(res$coords, x = res$dim, color_scheme = "PRGn")
  # Input weight function precision parameter with the Spectral palette
  embed_plot(res$coords, x = res$prec, color_scheme = "Spectral")

  # calculate the 32-nearest neighbor preservation for each observation
  # 0 means no neighbors preserved, 1 means all of them
  pres32 <- nbr_pres(res$dx, res$dy, 32)
  embed_plot(res$coords, x = pres32, cex = 1.5)
}
\dontrun{
# Do t-SNE on the iris dataset, scaling columns to zero mean and
# unit variance.
res <- sneer(iris, scale_type = "a")

# Use the weighted TSNE variant and export the input and output distance
# matrices.
res <- sneer(iris, scale_type = "a", method = "wtsne", ret = c("dx", "dy"))

# calculate the 32-nearest neighbor preservation for each observation
# 0 means no neighbors preserved, 1 means all of them
pres32 <- nbr_pres(res$dx, res$dy, 32)

# Calculate the Area Under the RNX Curve
rnx_auc <- rnx_auc_embed(res$dx, res$dy)

# Load the PRROC library
library(PRROC)

# Calculate the Area Under the Precision Recall Curve for the embedding
pr <- pr_auc_embed(res$dy, iris$Species)

# Similarly, for the ROC curve:
roc <- roc_auc_embed(res$dy, iris$Species)

# Load the RColorBrewer library
library(RColorBrewer)
# Plot the embedding, with points colored by the neighborhood preservation
embed_plot(res$coords, x = pres32, color_scheme = "Blues")
}
}
\references{
Cook, J., Sutskever, I., Mnih, A., & Hinton, G. E. (2007).
Visualizing similarity data with a mixture of maps.
In \emph{International Conference on Artificial Intelligence and Statistics} (pp. 67-74).

Hinton, G. E., & Roweis, S. T. (2002).
Stochastic neighbor embedding.
In \emph{Advances in neural information processing systems} (pp. 833-840).

Lee, J. A., Renard, E., Bernard, G., Dupont, P., & Verleysen, M. (2013).
Type 1 and 2 mixtures of Kullback-Leibler divergences as cost functions in
dimensionality reduction based on similarity preservation.
\emph{Neurocomputing}, \emph{112}, 92-108.

Lee, J. A., Peluffo-Ordo'nez, D. H., & Verleysen, M. (2015).
Multi-scale similarities in stochastic neighbour embedding: Reducing
dimensionality while preserving both local and global structure.
\emph{Neurocomputing}, \emph{169}, 246-261.

Van der Maaten, L., & Hinton, G. (2008).
Visualizing data using t-SNE.
\emph{Journal of Machine Learning Research}, \emph{9}(2579-2605).

Venna, J., Peltonen, J., Nybo, K., Aidos, H., & Kaski, S. (2010).
Information retrieval perspective to nonlinear dimensionality reduction for
data visualization.
\emph{Journal of Machine Learning Research}, \emph{11}, 451-490.

Vladymyrov, M., & Carreira-Perpinan, M. A. (2012).
Partial-Hessian Strategies for Fast Learning of Nonlinear Embeddings.
In \emph{Proceedings of the 29th International Conference on Machine Learning (ICML-12)}
(pp. 345-352).

Yang, Z., King, I., Xu, Z., & Oja, E. (2009).
Heavy-tailed symmetric stochastic neighbor embedding.
In \emph{Advances in neural information processing systems} (pp. 2169-2177).

Yang, Z., Peltonen, J., & Kaski, S. (2014).
Optimization equivalence of divergences improves neighbor embedding.
In \emph{Proceedings of the 31st International Conference on Machine Learning (ICML-14)}
(pp. 460-468).

t-SNE, SNE and ASNE
Van der Maaten, L., & Hinton, G. (2008).
Visualizing data using t-SNE.
\emph{Journal of Machine Learning Research}, \emph{9}(2579-2605).

NeRV
Venna, J., Peltonen, J., Nybo, K., Aidos, H., & Kaski, S. (2010).
Information retrieval perspective to nonlinear dimensionality reduction for
data visualization.
\emph{Journal of Machine Learning Research}, \emph{11}, 451-490.

JSE
Lee, J. A., Renard, E., Bernard, G., Dupont, P., & Verleysen, M. (2013).
Type 1 and 2 mixtures of Kullback-Leibler divergences as cost functions in
dimensionality reduction based on similarity preservation.
\emph{Neurocomputing}, \emph{112}, 92-108.

Nesterov Accelerated Gradient:
Sutskever, I., Martens, J., Dahl, G., & Hinton, G. (2013).
On the importance of initialization and momentum in deep learning.
In \emph{Proceedings of the 30th international conference on machine learning (ICML-13)}
(pp. 1139-1147).

O'Donoghue, B., & Candes, E. (2013).
Adaptive restart for accelerated gradient schemes.
\emph{Foundations of computational mathematics}, \emph{15}(3), 715-732.

Spectral Direction:
Vladymyrov, M., & Carreira-Perpinan, M. A. (2012).
Partial-Hessian Strategies for Fast Learning of Nonlinear Embeddings.
In \emph{Proceedings of the 29th International Conference on Machine Learning (ICML-12)}
(pp. 345-352).
}

